{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-15T03:02:20.432543700Z",
     "start_time": "2024-03-15T03:02:20.411436200Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from utils.activate import Swish\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "#残差模块\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    每个ResidualBlock都有两层CNN做特征提取\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,in_channels:int,out_channels:int,time_channels:int,n_groups:int=1,dropout:float=0.1):\n",
    "        \"\"\"\n",
    "        :param in_channels: 输入图片的Channel数量\n",
    "        :param out_channels: 经过残差块后输出的channel数量\n",
    "        :param time_channels:时间步的编码长度\n",
    "        :param n_groups: GroupNorm的超参数\n",
    "        :param dropout: dropout概率\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        #第一层卷积\n",
    "        self.norm1=nn.GroupNorm(n_groups,in_channels) #组归一化方式，将in_channels分成n_groups组然后再归一化\n",
    "        self.act1=Swish()\n",
    "        self.conv1=nn.Conv2d(in_channels,out_channels,kernel_size=(3,3),padding=(1,1)) #这样的卷积设置不改变图像大小，只改变通道数\n",
    "        \n",
    "        #第二层卷积是类似的\n",
    "        self.norm2=nn.GroupNorm(n_groups,out_channels)\n",
    "        self.act2=Swish()\n",
    "        self.conv2=nn.Conv2d(out_channels,out_channels,kernel_size=(3,3),padding=(1,1))\n",
    "        \n",
    "        #当in_channels与out_channels，残差连接直接将输入输出相加\n",
    "        #否则的话对输入做一次卷积\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut=nn.Conv2d(in_channels,out_channels,kernel_size=(1,1))\n",
    "        else:\n",
    "            self.shortcut=nn.Identity() #对输入不做任何操作直接输出\n",
    "        \n",
    "        #时间步t的编码维度有可能不等于out_channels所以要对时间步编码先进行一次线性转换\n",
    "        self.time_emb=nn.Linear(time_channels,out_channels)\n",
    "        self.time_act=Swish()\n",
    "        \n",
    "        self.dropout=nn.Dropout(p=dropout)\n",
    "    \n",
    "    def forward(self,x:torch.Tensor,t:torch.Tensor):\n",
    "        \"\"\"\n",
    "        \n",
    "        :param x: 输入数据xt，尺寸大小为(batch_size, in_channels, height, width)\n",
    "        :param t: 输入数据t，尺寸大小为(batch_size, time_c)\n",
    "        :return: \n",
    "        \"\"\"\n",
    "        \n",
    "        #输入图片先过第一层卷积\n",
    "        h=self.conv1(self.act1(self.norm1(x)))\n",
    "        \n",
    "        #先对图片做激活和线性变换，将时间步编码变为out_channels长度，然后将对应的数字和图片对应的channel相加\n",
    "        time_embedding=self.time_emb(self.time_act(t))\n",
    "        time_embedding=time_embedding.reshape(time_embedding.shape[0],time_embedding.shape[1],1,1)\n",
    "        print(time_embedding.shape)\n",
    "        h+=time_embedding\n",
    "        \n",
    "        #对图片再做一次卷积\n",
    "        \n",
    "        h=self.conv2(self.dropout(self.act2(self.norm2(h))))\n",
    "        \n",
    "        #处理一下原始输入\n",
    "        x_res=self.shortcut(x)\n",
    "        \n",
    "        #返回最终结果\n",
    "        return h+x_res\n",
    "        \n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T02:59:20.697717500Z",
     "start_time": "2024-03-15T02:59:20.687752Z"
    }
   },
   "id": "187ccebe4e40aa2"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 64, 1, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([[[[ 8.5606e-02, -7.7289e-01, -2.0768e-01,  ..., -1.0111e+00,\n           -1.5916e+00, -1.4278e-01],\n          [-7.4278e-01, -3.4155e-01,  5.2280e-01,  ...,  1.2547e+00,\n           -5.1153e-01, -6.0071e-01],\n          [-6.1843e-01, -6.8885e-01, -8.2031e-01,  ...,  7.1854e-01,\n            2.0468e-01, -1.2897e+00],\n          ...,\n          [-2.0762e+00, -3.4601e-01, -7.6262e-01,  ...,  9.0228e-02,\n           -2.1977e-01, -2.2416e-01],\n          [ 1.3652e+00, -1.9547e-01,  3.6606e-01,  ..., -9.6094e-01,\n           -5.3904e-01, -3.2842e-01],\n          [-5.2141e-01, -1.2282e-01, -8.5259e-01,  ..., -4.5303e-01,\n            1.0895e-01,  2.7173e-02]],\n\n         [[ 4.2157e-01, -4.1086e-01, -2.6723e-01,  ..., -3.5470e-01,\n           -1.7237e+00, -1.1213e+00],\n          [-9.7227e-01, -1.2354e+00,  8.4485e-01,  ...,  1.0630e+00,\n           -3.6348e-01, -4.5441e-01],\n          [-1.0432e-01, -3.8492e-01, -1.3210e+00,  ..., -1.0033e+00,\n           -5.6063e-01, -1.5128e+00],\n          ...,\n          [-9.3507e-01, -1.2126e+00,  2.0470e-01,  ...,  2.6783e-01,\n           -9.5472e-02, -2.7226e-01],\n          [ 3.3114e-01, -9.0533e-01,  3.4505e-01,  ..., -1.2912e-02,\n           -5.2119e-02,  1.6114e-01],\n          [-4.3671e-01, -6.5944e-01, -7.7333e-01,  ..., -9.8918e-01,\n           -1.4091e+00,  4.9152e-03]],\n\n         [[-3.4647e-01, -5.6379e-02,  1.6521e-01,  ...,  4.9232e-01,\n            1.0691e+00,  1.8412e-01],\n          [ 1.7145e-01,  1.9695e-01, -1.3772e+00,  ..., -1.2923e+00,\n           -1.9134e-03, -1.1376e-01],\n          [-6.6795e-01, -1.4733e-02,  8.3011e-01,  ..., -2.1731e-01,\n            1.5479e-02,  3.2842e-01],\n          ...,\n          [ 4.3990e-01,  1.7426e-01, -4.3462e-01,  ..., -5.5983e-01,\n           -1.5996e-02, -4.9189e-01],\n          [-8.0066e-01,  3.8027e-01, -4.5330e-01,  ..., -2.0134e-01,\n           -2.4954e-01, -8.2329e-01],\n          [-4.2364e-01,  1.7571e-02, -9.8934e-02,  ..., -1.8326e-01,\n            3.7926e-01, -7.1555e-01]],\n\n         ...,\n\n         [[ 5.2898e-01, -4.0705e-01,  1.5926e+00,  ...,  1.5280e+00,\n           -1.0868e+00,  2.2967e-01],\n          [-5.3900e-01, -4.5293e-01,  1.1504e+00,  ...,  1.1292e+00,\n           -2.1918e-01,  1.2363e+00],\n          [-2.1796e-01,  1.0334e+00, -1.5175e-01,  ..., -5.2193e-01,\n           -7.0437e-01, -7.6885e-01],\n          ...,\n          [-1.8196e-01, -3.7309e-01,  6.7519e-01,  ...,  9.8884e-01,\n            5.6397e-01, -5.7496e-01],\n          [ 1.0909e+00,  2.5451e-01,  1.9468e+00,  ...,  4.6960e-01,\n           -4.4994e-01, -8.1290e-02],\n          [-1.7420e-02, -7.3254e-02,  3.6516e-01,  ..., -1.1192e+00,\n           -9.9193e-02,  1.0281e+00]],\n\n         [[ 2.5831e-01,  6.7185e-01, -4.2363e-01,  ..., -1.2843e+00,\n            9.3021e-02,  2.7230e-01],\n          [ 1.3740e-01,  9.0773e-01,  6.3838e-01,  ...,  1.3100e+00,\n            3.8884e-01, -6.4428e-01],\n          [ 4.1624e-01, -4.0681e-01, -3.1784e-01,  ...,  9.9133e-01,\n            1.0732e+00, -5.2385e-01],\n          ...,\n          [-1.2628e+00,  4.4283e-01, -3.7073e-01,  ...,  2.3027e-01,\n           -4.6791e-02,  2.9597e-01],\n          [ 9.6478e-01,  3.6132e-01, -9.7300e-01,  ..., -1.2937e-01,\n            2.3528e-01,  3.1160e-01],\n          [ 9.5439e-02,  5.3458e-01, -4.0696e-01,  ...,  1.3793e+00,\n            9.6767e-01, -2.0907e-01]],\n\n         [[ 2.1460e-01, -8.0775e-02, -1.0984e+00,  ..., -7.5205e-01,\n           -1.3071e+00, -9.7492e-01],\n          [-1.1381e+00, -8.2249e-01,  1.2878e+00,  ...,  6.5445e-01,\n           -6.9932e-01,  1.9118e-01],\n          [ 1.5317e-01, -1.9957e-01, -1.5060e+00,  ..., -6.4070e-01,\n           -7.9193e-01, -5.0869e-01],\n          ...,\n          [-6.1497e-01, -6.9823e-01,  3.5460e-01,  ...,  3.0962e-01,\n           -3.5489e-01,  8.7628e-01],\n          [-6.2169e-01, -7.8932e-01,  2.4447e-01,  ..., -7.6917e-02,\n           -2.1696e-01,  1.2392e+00],\n          [ 9.4451e-02, -7.4711e-01, -3.7515e-01,  ..., -4.7428e-01,\n           -1.8710e+00,  4.7315e-01]]],\n\n\n        [[[-1.3371e+00, -1.0200e+00, -1.7671e+00,  ..., -5.8418e-03,\n           -4.2958e-01,  1.4031e+00],\n          [-1.8302e+00, -1.2813e+00, -3.2371e-02,  ..., -1.8315e-01,\n           -9.3723e-01, -1.0951e+00],\n          [-1.2750e+00, -3.9273e-01, -3.2384e-01,  ..., -1.6343e+00,\n           -2.4277e-01,  8.4871e-01],\n          ...,\n          [-7.5063e-01, -7.4176e-01, -9.8792e-01,  ..., -2.7092e-02,\n           -2.0023e+00,  4.9705e-01],\n          [-1.2970e+00, -4.0884e-01, -4.7762e-01,  ..., -9.0898e-01,\n           -9.4059e-01,  5.8862e-01],\n          [-4.1806e-01, -1.3176e+00, -8.4096e-01,  ..., -4.3874e-01,\n           -4.8880e-01, -3.3109e-01]],\n\n         [[-5.7913e-01, -1.0954e+00, -2.9365e-01,  ...,  4.9252e-02,\n            1.3134e-01,  7.7480e-01],\n          [-9.4450e-01, -7.7543e-01, -2.1251e-01,  ..., -5.1000e-01,\n           -6.2346e-01, -7.9483e-01],\n          [-1.4668e+00, -7.4595e-01, -1.0393e+00,  ..., -5.3460e-01,\n           -4.7782e-01, -7.8537e-01],\n          ...,\n          [ 4.7626e-02, -1.1237e+00, -2.2623e+00,  ..., -7.6166e-01,\n           -5.5700e-01, -5.5003e-01],\n          [-4.8999e-01, -7.1718e-01, -8.1884e-01,  ...,  3.8356e-01,\n           -5.4602e-02, -4.2338e-02],\n          [-1.5028e+00, -1.3711e+00, -6.3168e-01,  ..., -1.8044e+00,\n           -7.8090e-01,  1.3607e-02]],\n\n         [[-5.4009e-02,  3.2111e-01,  2.8428e-01,  ..., -6.9468e-01,\n           -5.7641e-01, -1.2435e+00],\n          [ 3.9385e-01,  4.3673e-01, -7.9224e-01,  ..., -3.1364e-01,\n           -4.0431e-01, -7.7979e-02],\n          [ 1.7175e-01, -6.1354e-01, -3.6596e-01,  ...,  2.3005e-02,\n           -7.9793e-01, -5.0257e-01],\n          ...,\n          [-6.9432e-01,  1.5441e-02, -5.2927e-03,  ..., -2.1484e-01,\n            1.2192e-02, -2.6064e-01],\n          [-2.9401e-01, -4.6763e-01, -1.2848e-01,  ..., -1.2325e-01,\n           -8.2836e-01, -3.1410e-01],\n          [ 6.0008e-02,  8.4197e-02,  2.7455e-02,  ..., -5.1134e-02,\n           -5.3749e-01, -2.9628e-01]],\n\n         ...,\n\n         [[-1.6304e-01,  9.5589e-01,  7.5682e-01,  ...,  4.8993e-01,\n            8.0365e-01,  1.6423e+00],\n          [ 5.9523e-01,  5.4002e-01, -5.8600e-01,  ...,  5.1145e-01,\n           -7.1869e-01, -5.4871e-02],\n          [-2.0809e+00, -5.2873e-01,  3.8587e-01,  ...,  2.9836e-01,\n           -6.4213e-01,  6.8767e-01],\n          ...,\n          [ 1.6002e+00, -9.0645e-01, -1.8080e+00,  ..., -7.6830e-01,\n            1.6541e+00, -1.2287e-01],\n          [ 3.5124e-01, -1.5608e-01,  1.3076e+00,  ...,  2.1961e+00,\n           -4.2068e-01,  1.4428e+00],\n          [-1.1744e-01,  4.7801e-02,  1.2745e+00,  ..., -9.9244e-01,\n           -5.1544e-01,  1.4112e+00]],\n\n         [[-1.2956e-01, -1.0500e+00, -1.1828e+00,  ...,  5.9318e-01,\n            1.4690e-01,  1.0330e+00],\n          [-9.8301e-01, -3.6276e-01,  9.4695e-01,  ...,  3.0467e-01,\n            3.8058e-01,  1.4134e-01],\n          [ 9.6546e-01,  1.2406e+00,  4.5767e-01,  ..., -6.5975e-01,\n            1.0480e+00,  1.2451e+00],\n          ...,\n          [-8.7167e-01,  8.7173e-01,  6.1601e-01,  ...,  1.1395e+00,\n           -1.8318e+00,  1.4455e+00],\n          [-6.7451e-01,  5.0348e-01, -3.6085e-01,  ..., -1.5699e+00,\n            6.4532e-01,  4.8387e-01],\n          [ 1.8732e-01, -3.0997e-01, -8.0098e-01,  ...,  1.2126e+00,\n            8.6750e-01, -5.3915e-01]],\n\n         [[ 4.1492e-01, -1.2915e-01,  4.1132e-01,  ...,  9.7766e-01,\n            5.5914e-01,  1.1532e+00],\n          [-2.5707e-01, -8.1478e-02,  1.7912e+00,  ...,  4.7202e-01,\n            1.1181e+00,  7.5649e-01],\n          [-1.1945e-01,  9.1449e-01, -2.5574e-01,  ...,  6.0878e-01,\n            1.0302e+00, -6.5223e-01],\n          ...,\n          [ 1.1012e+00,  1.7794e-01,  6.7080e-01,  ...,  7.4779e-01,\n            6.0408e-01,  8.7204e-01],\n          [ 8.6283e-01,  2.1844e-01, -4.8131e-01,  ...,  1.4266e+00,\n            6.3390e-01,  1.0728e-01],\n          [-5.7522e-01,  1.3863e-01,  3.8042e-01,  ..., -9.9001e-02,\n            1.2453e+00,  9.1999e-01]]],\n\n\n        [[[ 5.3972e-01,  2.1242e+00, -8.1582e-01,  ..., -3.4228e-01,\n            2.1034e-02, -4.9079e-01],\n          [ 7.6141e-01,  2.1607e-01,  1.5581e-02,  ..., -9.7692e-02,\n            1.5392e+00,  3.2686e-03],\n          [-1.1025e+00,  1.5001e+00, -2.0512e-01,  ...,  2.3347e-01,\n            1.8655e-01, -3.2841e-01],\n          ...,\n          [ 8.4169e-01, -2.4456e-01,  3.3656e-01,  ...,  8.5905e-02,\n           -6.1931e-01, -5.3556e-01],\n          [-8.1305e-01, -1.8209e-01,  1.6662e-01,  ..., -5.4883e-01,\n            4.6861e-01, -2.1871e-01],\n          [ 8.3715e-01, -1.3546e+00,  3.6094e-01,  ..., -1.7492e-02,\n            2.9650e-01, -1.9148e-01]],\n\n         [[-6.6927e-01,  1.1169e-01, -8.4236e-01,  ..., -1.2313e-01,\n           -3.3783e-01,  1.2526e+00],\n          [-8.8960e-02, -4.5261e-03, -5.2946e-01,  ..., -4.3379e-02,\n            2.3065e-01, -1.1038e+00],\n          [-8.1275e-01, -3.7763e-03, -3.7276e-01,  ...,  9.0538e-01,\n            3.3858e-01, -1.4676e+00],\n          ...,\n          [ 7.0557e-01, -9.2119e-01, -5.8482e-01,  ..., -4.8443e-01,\n           -1.7012e+00, -1.3878e+00],\n          [-1.9079e-01, -8.8233e-01,  7.0328e-01,  ..., -7.5034e-01,\n            8.4761e-01, -3.4217e-01],\n          [ 8.0949e-01, -1.4010e+00, -1.0504e+00,  ..., -2.4491e-01,\n            2.8027e-01, -9.8216e-01]],\n\n         [[ 3.5052e-01, -9.3841e-01,  2.6098e-01,  ...,  4.7993e-02,\n            2.1922e-01, -9.1805e-01],\n          [-4.3974e-01, -2.5379e-01, -4.3747e-01,  ..., -5.6570e-01,\n           -1.0275e+00, -3.7812e-01],\n          [ 6.0195e-01, -4.0859e-01, -1.8275e-01,  ..., -7.0024e-01,\n           -4.2914e-01,  1.4745e-01],\n          ...,\n          [-4.4992e-01,  4.6675e-01, -3.5357e-02,  ...,  4.6646e-02,\n            2.4773e-01,  6.2864e-01],\n          [-1.9457e-01, -6.6239e-01, -8.3919e-01,  ...,  4.9634e-01,\n           -2.5520e-01, -4.2771e-02],\n          [-1.0149e+00,  6.4109e-01,  2.2716e-02,  ..., -5.5923e-01,\n           -4.6547e-01, -3.2129e-01]],\n\n         ...,\n\n         [[ 8.1240e-01,  8.4990e-01,  2.1686e-01,  ...,  1.9877e+00,\n            1.2425e+00,  2.1133e+00],\n          [ 1.8696e+00,  9.0263e-01, -1.7681e-01,  ...,  4.4279e-01,\n            1.3919e+00, -9.8935e-01],\n          [-8.4824e-01,  1.5834e+00,  7.6936e-01,  ...,  1.1421e+00,\n            6.8566e-01,  7.7960e-02],\n          ...,\n          [ 1.6595e+00,  3.5478e-01,  1.1240e+00,  ...,  3.9612e-01,\n           -5.1917e-01,  1.5999e+00],\n          [ 2.2251e-01, -3.9198e-01,  9.3472e-01,  ...,  1.0845e-01,\n            2.4595e+00,  3.3306e-01],\n          [ 1.3271e+00, -8.5733e-01,  1.0435e+00,  ..., -5.6716e-01,\n            1.1607e-01, -7.4492e-01]],\n\n         [[ 2.7853e-01,  1.5470e+00, -5.2390e-03,  ..., -9.5214e-01,\n           -1.4080e-01, -6.5971e-01],\n          [-4.4692e-01, -8.1545e-02,  7.5606e-01,  ...,  5.3841e-01,\n            1.2892e+00,  1.6147e+00],\n          [-7.2945e-01,  2.3487e-01, -5.5552e-02,  ...,  1.2399e-01,\n            6.0172e-01,  1.7843e-01],\n          ...,\n          [-2.6157e-01,  3.6394e-01, -3.6406e-01,  ...,  1.0928e+00,\n            7.8096e-01, -8.5251e-01],\n          [-2.1739e-01,  7.0243e-01,  6.5880e-01,  ..., -1.1470e-01,\n           -8.6480e-01,  8.7578e-01],\n          [ 1.7797e-01, -3.7161e-01,  1.2523e-01,  ...,  7.9256e-01,\n            1.0694e+00,  1.0115e+00]],\n\n         [[-1.3023e+00, -7.0572e-02, -2.0849e-01,  ...,  3.9485e-01,\n           -3.5979e-01,  1.6014e+00],\n          [-3.8252e-01,  2.2607e-01,  6.4650e-01,  ...,  3.6649e-01,\n            2.8877e-02,  1.7863e-01],\n          [-8.4607e-01, -6.9078e-01, -2.1906e-01,  ...,  6.6541e-01,\n           -1.0515e-01, -1.1479e+00],\n          ...,\n          [ 3.7398e-01, -1.8302e+00,  4.2758e-01,  ..., -8.0684e-01,\n           -7.2273e-01, -1.0180e+00],\n          [ 7.5537e-01,  7.3468e-01,  6.7145e-01,  ..., -4.9380e-01,\n            8.5705e-02,  2.2612e-01],\n          [ 1.0744e+00, -4.7692e-01, -1.3903e+00,  ...,  6.8208e-01,\n            3.6092e-01, -3.5221e-02]]],\n\n\n        ...,\n\n\n        [[[-9.3279e-01, -9.2537e-01, -6.7551e-02,  ..., -5.3889e-02,\n           -4.8974e-02, -1.0157e+00],\n          [ 4.2970e-01, -2.3818e-01, -5.0500e-01,  ...,  1.1180e-01,\n           -7.1505e-01, -5.2836e-01],\n          [ 5.3904e-01, -1.1541e+00, -9.6648e-02,  ..., -4.8577e-01,\n            2.3136e-01, -7.4168e-01],\n          ...,\n          [ 2.2716e-01, -8.6686e-01, -2.3351e-01,  ..., -1.5261e-01,\n           -7.6732e-01,  4.1043e-01],\n          [ 2.3317e-01,  1.4474e+00,  1.8792e-01,  ..., -1.0842e+00,\n            1.9718e-02,  7.2098e-01],\n          [-4.8192e-01,  1.1766e+00, -1.5644e+00,  ...,  3.8253e-01,\n           -7.1067e-01, -1.8735e+00]],\n\n         [[-1.2437e+00, -2.3997e-01,  1.0656e-02,  ..., -1.0680e+00,\n            3.2871e-01, -8.5362e-01],\n          [ 1.4127e-01, -8.0060e-01, -3.0925e-01,  ...,  4.6668e-01,\n           -1.2819e-01, -1.0493e+00],\n          [-5.0841e-01, -8.8845e-01, -3.6537e-01,  ..., -5.4834e-01,\n           -4.1968e-01, -1.0475e+00],\n          ...,\n          [-7.7948e-01, -9.2005e-01, -3.1000e-01,  ..., -3.2426e-02,\n           -4.2206e-01,  3.9554e-01],\n          [-1.4236e-01,  1.6351e-01, -1.1954e+00,  ..., -2.5963e-01,\n           -1.7776e+00,  2.4449e-01],\n          [-1.4953e+00, -1.2216e+00, -5.7553e-01,  ..., -5.9106e-01,\n           -1.4980e+00, -1.8801e+00]],\n\n         [[ 6.3873e-01,  1.5147e-01, -5.8899e-01,  ...,  2.1540e-01,\n           -1.2207e-01,  4.9419e-01],\n          [-1.0151e+00,  4.1070e-02,  3.0575e-02,  ..., -7.7090e-01,\n           -1.9171e-01,  2.9335e-01],\n          [ 3.9816e-01, -2.7301e-01, -7.1108e-01,  ..., -3.6017e-01,\n           -9.3068e-01, -2.6584e-01],\n          ...,\n          [-4.5082e-01,  4.3537e-02, -9.7286e-01,  ..., -8.3680e-01,\n           -5.4367e-01, -8.3594e-01],\n          [-5.2140e-01, -1.5006e+00, -3.7886e-02,  ..., -6.4222e-02,\n            2.3052e-01, -9.7446e-01],\n          [ 2.8514e-01, -1.5678e-01,  6.4482e-02,  ..., -2.8441e-01,\n            2.5858e-01,  4.1935e-01]],\n\n         ...,\n\n         [[ 4.2189e-01,  8.6578e-01,  5.4298e-01,  ...,  7.4025e-01,\n            1.5996e+00,  4.0878e-01],\n          [ 2.5849e-01, -2.4036e-02,  6.3794e-01,  ...,  1.0364e+00,\n            6.9852e-01,  8.7264e-01],\n          [ 2.6635e-01, -6.3400e-01,  2.0856e-01,  ...,  4.3493e-01,\n           -8.3839e-01, -5.3333e-01],\n          ...,\n          [-4.1652e-01, -2.7155e-01, -8.1355e-01,  ..., -1.8909e-01,\n            2.2575e-01,  1.5588e+00],\n          [ 7.9899e-01,  4.7741e-01,  1.4102e-01,  ...,  6.8010e-01,\n           -7.2702e-01,  1.2634e+00],\n          [-8.0499e-01,  1.1679e+00,  6.5840e-01,  ...,  1.0202e+00,\n           -7.0092e-01, -1.4725e+00]],\n\n         [[-7.9847e-01, -4.0532e-01,  1.7197e-01,  ..., -4.0607e-01,\n           -1.2267e-01, -2.7306e-01],\n          [ 8.4343e-01,  6.2523e-01, -1.1084e+00,  ...,  3.0924e-01,\n           -3.0955e-01, -2.5039e-01],\n          [ 7.4069e-01, -2.1192e-01, -1.4918e-01,  ..., -3.1063e-01,\n            1.5804e+00,  9.6171e-01],\n          ...,\n          [ 7.6224e-01, -3.9606e-01,  1.1066e+00,  ...,  6.7718e-01,\n            5.0333e-01,  3.1503e-01],\n          [ 4.2895e-01,  1.6763e+00,  7.1852e-01,  ..., -8.1491e-01,\n            1.1306e+00,  9.1772e-01],\n          [ 4.9544e-01,  3.8178e-01, -1.2451e+00,  ...,  1.4259e-01,\n           -2.5471e-02,  7.0174e-02]],\n\n         [[-1.3819e+00,  3.1432e-02,  1.7105e-01,  ..., -1.4568e+00,\n           -2.2657e-05, -3.0763e-01],\n          [ 6.8060e-01, -4.2494e-01,  2.6581e-01,  ...,  1.2751e+00,\n           -7.7489e-02, -9.1536e-01],\n          [-1.0337e+00,  3.7101e-01,  9.3414e-01,  ..., -2.2386e-01,\n            5.0073e-01,  1.2956e-03],\n          ...,\n          [-1.2633e-01, -1.7292e-01,  1.7582e+00,  ...,  4.5338e-01,\n            3.0048e-01,  9.5925e-01],\n          [ 2.5367e-01,  8.5787e-01, -1.1340e+00,  ...,  8.2748e-01,\n           -1.3779e+00,  3.2468e-01],\n          [-8.9752e-01, -1.4920e+00, -3.8914e-02,  ..., -3.1406e-01,\n           -5.7655e-01, -6.6488e-03]]],\n\n\n        [[[-1.7994e+00, -7.8053e-01, -6.9878e-03,  ...,  8.4753e-02,\n           -4.0594e-01,  9.3938e-02],\n          [-4.5591e-01, -1.5804e+00, -1.0073e+00,  ...,  3.5013e-01,\n           -3.2523e-01, -2.2754e-02],\n          [-4.9180e-01, -1.1175e+00, -1.4373e+00,  ..., -1.6710e+00,\n           -4.1396e-01, -3.9001e-01],\n          ...,\n          [ 8.7810e-02,  1.2240e+00, -1.1359e+00,  ..., -8.3503e-01,\n           -4.3197e-01,  1.0466e-01],\n          [ 4.6425e-01, -1.3864e+00, -1.7922e+00,  ..., -1.7656e-01,\n           -7.0994e-01, -1.7428e+00],\n          [-9.2584e-01, -1.1189e-02, -3.2931e-01,  ...,  1.9991e-01,\n           -9.1240e-01, -2.7877e-01]],\n\n         [[-2.8118e-01,  1.3348e-01, -8.9919e-01,  ...,  3.7871e-01,\n           -3.0347e-01, -3.0445e-01],\n          [ 1.8871e-01, -4.6143e-01, -9.6906e-01,  ..., -2.4297e-01,\n            1.0278e+00,  2.8244e-01],\n          [ 5.0546e-01, -1.6037e+00, -2.9249e-01,  ..., -2.1958e+00,\n           -1.9469e+00, -1.3631e+00],\n          ...,\n          [-4.6111e-01,  9.4503e-01, -3.9887e-01,  ..., -5.6641e-01,\n           -4.3085e-01, -5.0680e-02],\n          [-4.7430e-01, -6.4141e-01, -1.0369e+00,  ..., -1.1175e-03,\n            2.6919e-01, -8.9502e-01],\n          [-4.9419e-01, -4.5812e-01, -8.6295e-01,  ..., -4.5646e-01,\n           -1.0623e+00, -1.1592e+00]],\n\n         [[ 1.6684e-01, -7.5967e-01, -6.1107e-01,  ..., -1.0699e-01,\n           -1.9927e-01, -2.6062e-01],\n          [-2.7229e-01, -4.2980e-03,  1.3738e-01,  ..., -1.5764e-01,\n           -8.2190e-01, -5.0952e-01],\n          [ 4.9319e-02,  3.4468e-01, -6.6001e-01,  ...,  3.7421e-01,\n            6.0602e-01,  5.1502e-01],\n          ...,\n          [ 6.2943e-01, -1.3254e+00, -1.0007e-01,  ...,  5.4822e-01,\n            1.0914e-01, -6.4175e-01],\n          [ 2.0455e-01,  2.0996e-01,  2.9191e-01,  ..., -2.0940e-01,\n           -6.1435e-01,  6.3304e-03],\n          [ 2.2238e-02, -3.8361e-01, -3.9991e-02,  ..., -6.7080e-01,\n            5.1828e-01, -1.7463e-02]],\n\n         ...,\n\n         [[-1.1861e-01,  1.2107e-01, -5.7782e-01,  ...,  1.4995e+00,\n           -3.3972e-01,  5.4016e-02],\n          [ 2.1381e-02,  7.7449e-02,  1.4201e-01,  ...,  9.8732e-01,\n            1.6757e+00,  1.5039e+00],\n          [ 2.5895e+00, -2.1576e+00,  8.7433e-01,  ..., -9.3041e-01,\n           -6.2876e-01, -7.0569e-01],\n          ...,\n          [ 1.0827e+00,  5.7559e-01,  1.0966e+00,  ...,  8.7083e-01,\n           -5.8065e-01,  8.7565e-01],\n          [ 1.0308e+00, -2.2928e-01, -3.4941e-01,  ...,  1.2088e+00,\n            7.1790e-02, -1.0756e+00],\n          [ 4.1516e-01, -2.8497e-01,  2.5196e-01,  ...,  3.6824e-02,\n           -1.4567e+00,  4.8963e-01]],\n\n         [[-5.3822e-01,  7.6516e-01,  1.2848e+00,  ..., -2.3653e-01,\n            2.7676e-01,  8.9347e-01],\n          [-2.1922e-01, -4.2517e-01, -6.5111e-01,  ...,  3.5061e-01,\n           -4.6474e-01, -2.5306e-01],\n          [-1.6797e+00,  8.1152e-01, -1.0381e+00,  ..., -9.9131e-01,\n            5.8794e-01,  1.1076e-02],\n          ...,\n          [-3.1031e-01,  5.7864e-01, -4.2745e-01,  ..., -3.8132e-01,\n            7.0307e-01,  7.4784e-01],\n          [-4.8439e-01, -9.3697e-01, -8.4691e-01,  ..., -1.8636e-01,\n           -1.3224e-01,  2.7837e-01],\n          [-7.0060e-01,  6.8673e-01, -1.8479e-02,  ...,  7.2161e-01,\n            1.9695e-01,  1.7427e-01]],\n\n         [[ 1.6687e-01,  6.6758e-01,  3.0880e-01,  ..., -5.6085e-01,\n            3.3415e-01,  3.4072e-01],\n          [ 1.0427e+00, -2.6985e-01,  4.7835e-01,  ..., -5.6066e-02,\n            1.2276e+00, -3.4202e-01],\n          [ 3.1090e-01,  6.0782e-03,  9.7251e-01,  ..., -7.0810e-02,\n           -6.2988e-01,  5.0496e-03],\n          ...,\n          [-1.2857e+00,  1.6193e+00, -4.9295e-02,  ..., -6.6936e-01,\n            5.2188e-01,  4.5290e-01],\n          [-8.6963e-01,  8.3917e-01,  7.7302e-01,  ...,  2.3825e-01,\n            1.3966e+00,  5.7424e-01],\n          [-4.2855e-01,  1.9974e-01, -3.0985e-01,  ..., -2.8338e-01,\n           -4.8360e-02, -6.6146e-01]]],\n\n\n        [[[ 3.9471e-01,  9.0178e-01,  1.0534e-02,  ...,  1.5763e+00,\n            6.5594e-02, -6.0304e-01],\n          [-1.0487e+00,  1.2015e+00,  4.6750e-01,  ..., -2.8258e-01,\n           -1.1178e+00, -6.2310e-01],\n          [-6.6701e-01, -2.4080e-02, -5.1756e-01,  ..., -1.5237e-01,\n            5.7019e-02, -1.6325e+00],\n          ...,\n          [ 4.7441e-01,  4.1154e-01,  5.6452e-01,  ...,  3.0863e-01,\n           -1.6032e-01, -5.5353e-02],\n          [ 4.6643e-01,  1.2006e-01,  2.2577e-01,  ...,  5.0878e-02,\n            7.4286e-01, -1.2083e+00],\n          [ 7.2542e-01,  7.7383e-02,  1.5275e-01,  ...,  4.6029e-01,\n            1.5390e+00, -1.2607e+00]],\n\n         [[-1.2016e+00,  6.5513e-01, -7.0534e-01,  ..., -7.5740e-01,\n           -1.4043e-01, -6.1077e-01],\n          [-1.0606e+00,  5.1067e-01,  1.5248e-01,  ...,  7.4654e-01,\n           -6.6443e-01, -4.1035e-01],\n          [-9.4779e-01, -6.9285e-02, -4.3420e-01,  ...,  4.2830e-02,\n            4.0253e-01, -2.1669e+00],\n          ...,\n          [ 1.4448e-01,  4.0364e-02,  3.9281e-01,  ..., -1.0662e+00,\n           -3.6808e-01, -5.9687e-01],\n          [ 1.9359e-02, -4.9359e-02, -1.4340e-01,  ..., -4.4799e-01,\n            1.8188e-01, -8.9372e-01],\n          [ 4.9641e-02, -5.1048e-01, -3.8201e-01,  ..., -7.9131e-01,\n           -3.0614e-01, -5.4375e-01]],\n\n         [[ 3.5953e-01, -1.2436e+00, -2.5895e-01,  ..., -7.2551e-01,\n           -2.3277e-01, -1.0037e-01],\n          [ 4.3822e-01, -7.9021e-01, -5.3592e-01,  ..., -6.5955e-01,\n           -4.2148e-01, -1.1433e+00],\n          [ 1.4505e-02, -3.4078e-01, -3.7985e-01,  ..., -2.6675e-01,\n           -7.4080e-01,  8.0100e-01],\n          ...,\n          [-3.5536e-01, -3.6640e-01, -9.8906e-01,  ..., -5.4167e-02,\n           -1.4550e-01, -7.0992e-01],\n          [-4.6628e-01, -9.0944e-01, -1.5442e-01,  ..., -4.5060e-01,\n           -7.9859e-01, -5.2628e-03],\n          [-6.1999e-01, -2.5953e-01, -5.3647e-01,  ..., -1.1923e-01,\n           -6.6254e-01, -7.2711e-02]],\n\n         ...,\n\n         [[ 5.2827e-01,  6.5469e-01,  3.6776e-03,  ..., -1.4369e+00,\n            1.4445e+00, -1.6738e-02],\n          [ 1.0661e+00,  2.2176e+00,  7.0924e-01,  ...,  2.0780e+00,\n           -1.0519e+00, -3.7944e-01],\n          [ 8.4649e-02,  2.7309e-01,  6.6576e-02,  ...,  5.6009e-01,\n            6.0496e-01, -1.9607e+00],\n          ...,\n          [ 5.3692e-01, -4.8011e-01,  1.2645e+00,  ...,  5.1794e-01,\n           -1.5209e-01, -8.3791e-02],\n          [-1.5594e-01,  9.3171e-01,  5.0715e-01,  ...,  5.5033e-01,\n           -1.7827e-01,  5.0500e-01],\n          [ 1.2008e+00,  4.7210e-01,  1.0018e-01,  ...,  4.7113e-01,\n            3.4144e-01, -3.7510e-02]],\n\n         [[ 2.0041e-01,  8.3619e-01,  6.7578e-01,  ...,  2.1956e+00,\n            9.0437e-02,  4.8525e-01],\n          [-1.2515e+00, -2.8451e-01,  3.6791e-01,  ..., -1.3272e+00,\n            6.7897e-01,  7.3412e-01],\n          [-1.7793e-01,  3.6652e-01, -8.6415e-03,  ..., -3.7376e-01,\n            1.0972e-01,  2.2538e-01],\n          ...,\n          [ 5.5291e-01,  9.7955e-01,  5.8736e-01,  ...,  5.1039e-01,\n            8.7990e-01,  9.2060e-01],\n          [ 9.6784e-01,  3.7139e-01,  4.2232e-01,  ...,  2.1762e-01,\n            1.0210e+00, -7.3281e-01],\n          [ 4.3923e-01,  5.1041e-01,  6.8167e-01,  ...,  6.2395e-01,\n            1.4041e+00, -2.6140e-01]],\n\n         [[-1.0873e+00,  1.3915e+00, -4.6526e-01,  ...,  6.4828e-01,\n           -1.0964e+00, -2.8429e-01],\n          [-5.8839e-01,  2.5940e-01,  6.0495e-02,  ...,  8.3952e-01,\n            6.9046e-01,  5.9173e-01],\n          [ 4.5430e-02,  1.0025e+00,  2.0032e-01,  ...,  3.6958e-01,\n            2.1906e-01, -1.2197e+00],\n          ...,\n          [-2.9341e-01,  4.7009e-01,  6.0993e-01,  ..., -1.4005e+00,\n           -7.2376e-02,  7.9319e-02],\n          [ 8.5187e-01,  2.8091e-01, -1.9401e-01,  ...,  2.8476e-01,\n            6.3804e-01, -5.6780e-01],\n          [ 1.4165e-01, -6.2430e-01,  2.5236e-01,  ..., -9.6736e-01,\n           -1.3945e-01, -1.4723e-01]]]], grad_fn=<AddBackward0>)"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "residual_block=ResidualBlock(in_channels=3,out_channels=64,time_channels=32)\n",
    "pic_tensor=torch.randn(size=[128,3,224,224])\n",
    "time_embedding=torch.randn(size=(128,32))\n",
    "residual_block(pic_tensor,time_embedding)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T02:59:29.566615Z",
     "start_time": "2024-03-15T02:59:21.334684900Z"
    }
   },
   "id": "d37d7486b62f1c64"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "#attention模块\n",
    "class AttentionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    和Transformer中的多头注意力机制的原理以及实现方式一致\n",
    "    \"\"\"\n",
    "    def __init__(self,n_channels:int,n_heads:int=1,d_k:int=None,n_groups:int=32):\n",
    "        \"\"\"\n",
    "        \n",
    "        :param n_channels: 等待做注意力操作特征图的通道数\n",
    "        :param n_heads: 注意力头的数量\n",
    "        :param d_k: 每一个注意力头处理向量的维度\n",
    "        :param n_groups: Group Norm的超参数\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        #一般来言d_k=num_channels//num_heads,要保证num_channels可以被num_heads整除\n",
    "        if d_k is None:\n",
    "            d_k=n_channels\n",
    "        \n",
    "        #定义Group Norm层\n",
    "        self.norm=nn.GroupNorm(n_groups,n_channels)\n",
    "        \n",
    "        #多头注意力层，定义输入token和q,k,v矩阵相乘后的结果\n",
    "        self.projection=nn.Linear(n_channels,n_heads*d_k*3)\n",
    "        \n",
    "        #MLP层\n",
    "        self.output=nn.Linear(n_heads*d_k,n_channels)\n",
    "        \n",
    "        self.scale=d_k**-0.5  #求d_k平方根的倒数\n",
    "        self.n_heads=n_heads\n",
    "        self.d_k=d_k\n",
    "    \n",
    "    def forward(self,x:torch.Tensor,t: Optional[torch.Tensor]=None):\n",
    "        \"\"\"\n",
    "        \n",
    "        :param x: 输入数据xt，尺寸大小为(batch_size, in_channels, height, width)\n",
    "        :param t: 输入数据t，尺寸大小为(batch_size, time_c)\n",
    "        :return: \n",
    "        \"\"\"\n",
    "        \n",
    "        #其实并没有用到t\n",
    "        _=t\n",
    "        \n",
    "        #获取shape\n",
    "        batch_size,n_channels,height,width=x.shape\n",
    "        \n",
    "        #修改输入数据的形状，将形状修改为(batch_size,height*width,num_channels)\n",
    "        #这三个维度分别等同于Transformer中的(batch_size,seq_length,token_embedding)\n",
    "        #其实就是[batch_size,number_of_queries,query_size]\n",
    "        x=x.reshape(batch_size,height*width,-1)\n",
    "        #做完self.projection(x),形状变为(batch_size,height*width,n_heads*d_k*3)\n",
    "        qkv=self.projection(x).reshape(batch_size,-1,self.n_heads,3*self.d_k)\n",
    "        #把结果给切开 q,k,v的形状都是[batch_size,height*weight,num_heads,d_k]\n",
    "        q,k,v=torch.chunk(qkv,3,dim=-1)\n",
    "        \n",
    "        #计算注意力机制的过程\n",
    "        #想一下batch_size是不动的，对于q来讲有i个query，h个头，每个头维度是d\n",
    "        #对v来讲，有j个value，h个头，每个头的维度是d\n",
    "        #那么计算q和v的相似度其实是每个query和每个value的每个头计算一下相似度\n",
    "        #结果是[batch_size,number_of_queries,number_of_values,number_of_heads]\n",
    "        attn=torch.einsum('bihd,bjhd->bijh',q,k)*self.scale\n",
    "        \n",
    "        #所以这里是沿着第二维度做softmax操作\n",
    "        attn=attn.softmax(dim=2)\n",
    "        \n",
    "        #接下来得到相似度之后就要根据softmax的结果加权了\n",
    "        #对于每个value按照number_of_head中的权重加权\n",
    "        #也就是batch_size不动，number_of_queries不动,value按照相似度加权\n",
    "        res=torch.einsum('bijh,bjhd->bihd',attn,v)\n",
    "        \n",
    "        \n",
    "        res=res.reshape(batch_size,-1,self.n_heads*self.d_k)\n",
    "        \n",
    "        #再过一层MLP\n",
    "        res=self.output(res)\n",
    "        \n",
    "        #做个残差连接\n",
    "        res+=x\n",
    "        \n",
    "        res=res.permute(0,2,1).reshape(batch_size,n_channels,height,width)\n",
    "        \n",
    "        return res"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T03:57:27.147635500Z",
     "start_time": "2024-03-15T03:57:27.138666900Z"
    }
   },
   "id": "3933991d7b235578"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#DownBlock模块\n",
    "class DownBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self,in_channels:int,out_channels:int,time_channels:int,has_atten:bool):\n",
    "        super().__init__()\n",
    "        self.res=ResidualBlock(in_channels,out_channels,time_channels)\n",
    "        if has_atten:\n",
    "            self.attn=AttentionBlock(out_channels)\n",
    "        else:\n",
    "            self.attn=nn.Identity()\n",
    "    \n",
    "    def forward(self,x:torch.Tensor,t:torch.Tensor):\n",
    "        x=self.res(x,t)\n",
    "        x=self.attn(x)\n",
    "        \n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a8b37f91b0b51d1a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
